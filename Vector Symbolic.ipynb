{
 "metadata": {
  "name": "Vector Symbolic"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Embedding symbols into fixed-length vectors"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Any two random vectors in the unit sphere (with sufficient dimensionality) are nearly orthogonal. This also applies to most combinations of such random vectors. What if we choose to represent symbols (letters, words, whatever) as high dimensional vectors, and then combine them such that the resulting vectors are 1) distinct and 2) similar in some way to their constituents? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from seqVSA import *\n",
      "\n",
      "# VSA object for storage and computation\n",
      "\n",
      "myVSA = VSA(1000) # Choosing to represent atomic symbols as 1000-dimensional vectors\n",
      "\n",
      "# Let's embed some symbols\n",
      "\n",
      "myVSA.encode_conv(\"A\")\n",
      "myVSA.encode_conv(\"B\")\n",
      "myVSA.encode_conv(\"Z\")\n",
      "\n",
      "# If this works, AB = A + B should be more similar to A and B than to Z \n",
      "\n",
      "A, B, Z = myVSA.sequences[\"A\"], myVSA.sequences[\"B\"], myVSA.sequences[\"Z\"]\n",
      "\n",
      "AB = A + B # Element-wise addition operation. This should preserve membership (A is in AB) but not preserve order, since summation is commutative (AB = BA) \n",
      "\n",
      "# On to membership queries. We expect unrelated symbols to be nearly orthogonal and so have a lower dot product. This provides a naive way of doing set membership stuff\n",
      "\n",
      "np.dot(AB, A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "101442.26905362314"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(AB, B)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "97147.666190570104"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(AB, Z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "1727.6880027860061"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We can even encode structure into these representations by so-called binding operations. \n",
      "# These are specific to the choice of vectors (binary vectors (0,1), ternary vectors (-1,1) and real vectors [-1,1] all have\n",
      "# different canonical operations in the literature, and it's not quite clear which is preferable. \n",
      "# Here I have implemented real vectors and binding by circular convolution. We can encode sequential structure using this operation, \n",
      "# such that we can embed the sequence \"AB\" in the space so it is different from the orderless version"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myVSA.encode_conv(\"AB\")\n",
      "\n",
      "AB_seq = myVSA.sequences[\"AB\"]\n",
      "\n",
      "# This preserves set membership but also structure"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(A, AB_seq) > np.dot(Z, AB_seq)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# If two vectors are identical, they agree on all positions in the individual vectors, and this operation should return the dimensionality. If they're wholly distinct it returns 0\n",
      "sum(AB == AB_seq)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BA = B + A \n",
      "sum(AB == BA)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "1000"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sequences embedded in this space preserve some traditional notions of similarity, which I find highly interesting\n",
      "\n",
      "# String 1\n",
      "myVSA.encode_conv(\"GGCCTGCTATAA\")\n",
      "# String 2\n",
      "myVSA.encode_conv(\"GGCCTATAA\")\n",
      "# String 3\n",
      "myVSA.encode_conv(\"TTTGCGTAA\")\n",
      "\n",
      "# Intuitively, string 1 and 2 should be closest given the highest degree of overlap (difference is 3 deletions in the middle)\n",
      "\n",
      "string_1, string_2, string_3 = myVSA.sequences[\"GGCCTGCTATAA\"], myVSA.sequences[\"GGCCTATAA\"], myVSA.sequences[\"TTTGCGTAA\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(string_1, string_2), np.dot(string_1, string_3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "(724242.01523062354, 286233.08406682627)"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Where to go from here? My implementation doesn't normalize vectors after addition or binding, which is wrong. Also circular convolution isn't very efficient. \n",
      "# I also just pulled the sequence encoding method out of thin air - for every position in the sequence I make a distinct symbol, and binding the sequence \"ABC\" \n",
      "# amounts to bind(\"A\", \"pos1\") + bind(\"B\", pos2) + bind(\"C\", pos3). You could think of a million way of doing this. Other people have used binary vectors and random permutations\n",
      "# to encode sequential structure, maybe that works better? \n",
      "\n",
      "# In summary, this stuff has the potential (don't know how big that potential is though) to encode sequences _of arbitrary length_ into fixed-size vectors, in a way that preserves, in some sense,\n",
      "# \"what the sequence means\" - e.g. the vectors concord with the intuition you have of Smith-Waterman similarity or some such. \n",
      "\n",
      "# Immediate application: The Johnson-Lindenstrauss lemma states that there exists a space of much lower dimensionality that preserves the structure of the representation space. So one can potentially\n",
      "# reduce the dimensionality and visualize strings. One can also find a binary code for each represented string with a Deep Autoencoder (see Hinton and Salatkhudinov 2009) which enables Nearest Neighbor search\n",
      "# in O(1) if implemented in a RAM bus. Maybe you could do something like BLAST search much faster? \n",
      "\n",
      "# You could also encode Gene Ontology hierarchies, trees, or any discrete data structures if you were fanciful enough. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}